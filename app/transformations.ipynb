{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d83a5aa-30a2-458a-89b6-b9df7c0d5f69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Install Azure Event Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0985836e-c0e2-4723-b6a9-cd974e630f43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting azure-eventhub\n  Downloading azure_eventhub-5.11.5-py3-none-any.whl (315 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 315.5/315.5 kB 5.3 MB/s eta 0:00:00\nCollecting azure-core<2.0.0,>=1.14.0\n  Downloading azure_core-1.29.6-py3-none-any.whl (192 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 192.5/192.5 kB 17.9 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from azure-eventhub) (4.3.0)\nCollecting typing-extensions>=4.0.1\n  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (2.28.1)\nCollecting anyio<5.0,>=3.0\n  Downloading anyio-4.2.0-py3-none-any.whl (85 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.5/85.5 kB 10.3 MB/s eta 0:00:00\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.16.0)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.10/site-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.14.0->azure-eventhub) (3.3)\nCollecting sniffio>=1.1\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nCollecting exceptiongroup>=1.0.2\n  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.14.0->azure-eventhub) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.14.0->azure-eventhub) (2022.9.14)\nInstalling collected packages: typing-extensions, sniffio, exceptiongroup, anyio, azure-core, azure-eventhub\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8434981a-2377-4785-a388-14064f08635a\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\nSuccessfully installed anyio-4.2.0 azure-core-1.29.6 azure-eventhub-5.11.5 exceptiongroup-1.2.0 sniffio-1.3.0 typing-extensions-4.9.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb087076-1870-49d4-8da6-e78675dfb157",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b466cf8-0290-48ed-8986-c4c32fbc7b3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace 'your_database_name' with the desired database name\n",
    "database_name = 'formerSalamendars'\n",
    "\n",
    "# Create the database\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# Set the current database to the newly created database\n",
    "spark.sql(f\"USE {database_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daf41b6e-adb0-43fb-812f-0712f3039983",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fetch data from Azure Event Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f449b00c-c2f2-4295-becc-761062ef4f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, date_format, regexp_extract, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"movies-ratings-app\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.21\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "eventHubName = \"formersalamandersdata\"\n",
    "\n",
    "connection_string = f\"Endpoint=sb://formersalamanders.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=609Z/WJbdIVvaXu82BnZtLSDtDOUpcNPI+AEhC0b1GY=;EntityPath={eventHubName}\"\n",
    "\n",
    "ehConf = {}\n",
    "\n",
    "ehConf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "# ehConf[\"eventhubs.startingPosition\"] = -1\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"eventhubs\") \\\n",
    "  .options(**ehConf) \\\n",
    "  .load()\n",
    "\n",
    "selected = df.withColumn(\"body\", F.col(\"body\").cast(\"string\"))\n",
    "\n",
    "# Define columns\n",
    "columns = [\n",
    "    \"timestamp\", \"log_level\", \"request_id\", \"session_id\", \"user_id\", \"action\",\n",
    "    \"http_method\", \"url\", \"referrer_url\", \"ip_address\", \"user_agent\", \"response_time\",\n",
    "    \"product_id\", \"cart_size\", \"checkout_status\", \"token\", \"auth_method\", \"auth_level\",\n",
    "    \"correlation_id\", \"server_ip\", \"port_number\", \"protocol\", \"status\", \"detail\"\n",
    "]\n",
    "\n",
    "# Split the body column based on the delimiter\n",
    "split_df = df.select(F.split(F.col(\"body\"), \" \\| \").alias(\"split_data\"))\n",
    "\n",
    "# Extract columns and alias them\n",
    "for i in range(len(columns)):\n",
    "    split_df = split_df.withColumn(columns[i], split_df[\"split_data\"][i])\n",
    "\n",
    "# Select the required columns\n",
    "log_df = split_df.select(columns)\n",
    "\n",
    "# 1. Extract date and time into separate columns\n",
    "log_df = log_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\"))\n",
    "log_df = log_df.withColumn(\"date\", date_format(\"timestamp\", \"yyyy-MM-dd\"))\n",
    "log_df = log_df.withColumn(\"time\", date_format(\"timestamp\", \"HH:mm:ss.SSS\"))\n",
    "\n",
    "\n",
    "\n",
    "# 2. Extract domain from referrer_url\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "log_df = log_df.withColumn(\"referrer_domain\", regexp_extract(log_df[\"referrer_url\"], \"(?<=//)(.*?)(?=/|$)\", 1))\n",
    "\n",
    "\n",
    "# 3. Remove the name part from each column\n",
    "columns=['ip_address', 'user_agent', 'response_time', 'product_id', 'cart_size', 'checkout_status', 'token', 'auth_method', 'auth_level', 'correlation_id', 'server_ip', 'port_number', 'protocol', 'Detail']\n",
    "for column in columns:\n",
    "    log_df = log_df.withColumn(column, regexp_extract(col(column), r\"(?<=:\\s)(.*)\", 1))\n",
    "    \n",
    "\n",
    "# 4. Drop the 'timestamp' and 'referrer_url' columns\n",
    "columns_to_drop = ['timestamp', 'referrer_url']\n",
    "log_df = log_df.drop(*columns_to_drop)    \n",
    "\n",
    "\n",
    "# 5. Reorder columns in the DataFrame\n",
    "desired_columns_order = [\n",
    "    \"date\", \"time\",\"log_level\", \"request_id\", \"session_id\", \"user_id\", \"action\",\n",
    "    \"http_method\", \"url\", \"referrer_domain\", \"ip_address\", \"user_agent\", \"response_time\",\n",
    "    \"product_id\", \"cart_size\", \"checkout_status\", \"token\", \"auth_method\",\n",
    "    \"auth_level\", \"correlation_id\", \"server_ip\", \"port_number\", \"protocol\",\n",
    "    \"status\", \"detail\"\n",
    "]\n",
    "log_df = log_df.select(desired_columns_order)\n",
    "\n",
    "# Define the Hive table name and location\n",
    "hive_table_name = 'formerSalamendars_logs'\n",
    "table_location = '/mnt/former_salamenders/logs'\n",
    "\n",
    "# Write the streaming DataFrame to a location\n",
    "query = log_df.writeStream \\\n",
    "    .format('parquet') \\\n",
    "    .outputMode('append') \\\n",
    "    .option('path', table_location) \\\n",
    "    .option('checkpointLocation', '/mnt/checkpoint/dir') \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb78585-1ecb-4bf2-86df-11fce863e886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "pwd"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2891479621148815,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "transformations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
